# Transformers

The previous RNN/GRU/LSTM networks processed words for a translation in sequence.

Transformers are a way of processing the translation of every word in parallel.

It works by combining the "Attention" based model with the convolution network style of processing.

## Self-Attention
An attention based vector is determined for each word -> A(q, K, V) eg:

$$
L = \frac{1}{2} \rho v^2 S C_L
$$